# Projet Big Data â€“ Pipeline ELT, Analytics & NoSQL

Pipeline de donnÃ©es complÃ¨te pour un cas e-commerce avec architecture **Bronze / Silver / Gold**, orchestration **Prefect**, stockage **MinIO**, **MongoDB**, **API FastAPI** et **dashboard Streamlit**.

---

## ğŸš€ Setup du projet (aprÃ¨s git clone/pull)

### 1. PrÃ©requis

- **Python 3.12**
- **Docker** + **docker-compose**
- **Git**

### 2. Installation

```bash
# Cloner le repo (si pas dÃ©jÃ  fait)
git clone <url-du-repo>
cd big-data

# CrÃ©er l'environnement virtuel
python3 -m venv .venv
source .venv/bin/activate  # Sur Windows: .venv\Scripts\activate

# Installer les dÃ©pendances
pip install --upgrade pip
pip install -r requirements.txt
pip install -r api/requirements.txt
```

### 3. DÃ©marrer l'infrastructure Docker

```bash
# DÃ©marrer MinIO, Prefect, Postgres, MongoDB, Mongo Express
docker-compose up -d

# VÃ©rifier que tout est dÃ©marrÃ©
docker ps
```

**Services disponibles :**
- **MinIO** : `http://localhost:9000` (Console: `http://localhost:9001`, user: `minioadmin` / `minioadmin`)
- **Prefect UI** : `http://localhost:4200`
- **MongoDB** : `localhost:27017`
- **Mongo Express** : `http://localhost:8081` (user: `admin` / `admin`)

### 4. GÃ©nÃ©rer les donnÃ©es de test

```bash
python scripts/generate_data.py
```

Cela crÃ©e `data/sources/clients.csv` et `data/sources/achats.csv`.

### 5. ExÃ©cuter la pipeline ELT complÃ¨te

```bash
# 1. Ingestion Bronze (CSV -> MinIO bronze)
python flows/bronze_ingestion.py

# 2. Transformation Silver (nettoyage, normalisation)
python flows/silver_transformation.py

# 3. AgrÃ©gations Gold (KPIs, dimensions, faits)
python flows/gold_aggregation.py
```

### 6. Synchroniser Gold â†’ MongoDB

```bash
python -m mongo_pipeline.mongo_writer
```

### 7. Lancer l'API FastAPI

```bash
# Dans un terminal sÃ©parÃ©
uvicorn api.app:app --reload --port 8000
```

**API disponible :** `http://localhost:8000`
- Docs Swagger : `http://localhost:8000/docs`
- Endpoints : `/kpis/{name}`, `/facts/{name}`, `/analytics/{name}`, `/meta/last-refresh`

### 8. Lancer le dashboard Streamlit

```bash
# Dans un autre terminal
streamlit run dashboard/app.py
```

**Dashboard disponible :** `http://localhost:8501`

Dans la sidebar, tu peux :
- Choisir la source : **"MinIO (direct)"** ou **"API Mongo"**
- Mesurer les temps de chargement avec le bouton **"Mesurer MinIO vs API"**

---

## ğŸ“‹ Ce qui a Ã©tÃ© implÃ©mentÃ©

### âœ… 1. Pipeline ELT robuste (Bronze / Silver / Gold)

**Objectif :** Ingestion, nettoyage et agrÃ©gation des donnÃ©es e-commerce.

**FonctionnalitÃ©s :**
- **Bronze** : Ingestion gÃ©nÃ©rique (dÃ©couverte automatique de tous les CSV dans `data/sources/`)
- **Silver** : Nettoyage intelligent gÃ©nÃ©rique + cleaners spÃ©cifiques pour `clients`/`achats`
  - Gestion des valeurs nulles/aberrantes
  - Standardisation des dates
  - Normalisation des types
  - DÃ©duplication
  - ContrÃ´les de qualitÃ© (complÃ©tude, unicitÃ©, validitÃ©)
- **Gold** : AgrÃ©gations mÃ©tier complÃ¨tes
  - Dimensions : `dim_clients`, `dim_produits`, `dim_temps`, `dim_rfm`
  - Faits : CA par jour/semaine/mois/heure/pays
  - KPIs : CLV, rÃ©tention, churn, RFM, croissance, performance produits
  - Analytics : saisonnalitÃ©, concentration (Pareto/Gini), cohortes

**Fichiers clÃ©s :**
- `flows/bronze_ingestion.py`
- `flows/silver_transformation.py`
- `flows/gold_aggregation.py`
- `flows/transformations/` (nettoyage)
- `flows/aggregations/` (agrÃ©gations)

---

### âœ… 2. Machine Learning pour enrichissement automatique

**Objectif :** Extraire automatiquement plus d'informations sans rÃ¨gles mÃ©tier explicites.

**FonctionnalitÃ©s :**
- **Feature engineering automatique** : dÃ©tection de patterns (dates, numÃ©riques, catÃ©gorielles)
- **DÃ©tection d'anomalies** : Isolation Forest (10% d'anomalies dÃ©tectÃ©es)
- **Clustering automatique** : K-Means pour segmentation (10 clusters)
- **Scores prÃ©dits** : score composite (0-100) par client/achat

**Fichiers clÃ©s :**
- `flows/ml/feature_engineering.py`
- `flows/ml/ml_models.py`
- IntÃ©grÃ© dans `flows/gold_aggregation.py`

**RÃ©sultat :** Les donnÃ©es Gold sont enrichies avec des colonnes ML (`is_anomaly_ml`, `ml_cluster`, `ml_score`) sans casser les KPIs existants.

---

### âœ… 3. Base NoSQL opÃ©rationnelle avec MongoDB

**Objectif :** Pipeline Gold â†’ MongoDB + API + Dashboard avec comparaison des temps.

**FonctionnalitÃ©s :**
- **Pipeline MongoDB** (`mongo_pipeline/mongo_writer.py`) :
  - Lit tous les Parquet Gold depuis MinIO
  - Ã‰crit dans des collections Mongo (`kpis_*`, `facts_*`, `analytics_*`)
  - Mesure et stocke le temps de refresh Gold â†’ Mongo
- **API FastAPI** (`api/app.py`) :
  - Endpoints REST : `/kpis/{name}`, `/facts/{name}`, `/analytics/{name}`
  - Endpoint mÃ©tadonnÃ©es : `/meta/last-refresh`
- **Dashboard Streamlit** :
  - **Switch de source** : MinIO direct OU API Mongo (mÃªmes donnÃ©es)
  - **Comparaison des temps** : bouton dans la sidebar pour mesurer MinIO vs API
  - Aucun changement de logique mÃ©tier, juste deux modes de lecture

**Fichiers clÃ©s :**
- `mongo_pipeline/mongo_writer.py`
- `api/app.py`
- `dashboard/utils/data_loader.py` (deux modes : MinIO + API)
- `dashboard/app.py` (switch dans la sidebar)

**RÃ©sultat :** Tu peux comparer directement dans Streamlit les temps de chargement MinIO vs API Mongo pour exactement les mÃªmes donnÃ©es.

---

### âœ… 4. Dashboard Streamlit complet

**Objectif :** Visualisation interactive de tous les KPIs et analyses.

**Pages disponibles :**
- ğŸ  **Accueil** : KPIs globaux (CA, clients, achats, panier moyen)
- ğŸ“ˆ **Ã‰volution temporelle** : CA par jour/semaine/mois/heure
- ğŸŒ **Analyse gÃ©ographique** : CA par pays, top 10, distribution
- ğŸ¯ **Segmentation RFM** : Clients par segment (Champions, Loyal, etc.)
- ğŸ’° **Customer Lifetime Value** : CLV moyen par pays
- ğŸ”„ **RÃ©tention & Churn** : Taux de rÃ©tention, rÃ©currence, churn
- ğŸ“¦ **Performance produits** : Top produits par CA, volume
- ğŸ“… **SaisonnalitÃ©** : Patterns par jour/heure/mois de la semaine
- ğŸ“Š **Analyses avancÃ©es** : Concentration (Gini), cohortes

**Fichiers clÃ©s :**
- `dashboard/app.py`
- `dashboard/utils/data_loader.py`

---

## ğŸ—ï¸ Architecture technique

### Stack

- **Python 3.12** + **Pandas** + **PyArrow** (traitement batch)
- **Prefect** (orchestration)
- **MinIO** (data lake S3-compatible)
- **MongoDB 7** (base NoSQL opÃ©rationnelle)
- **FastAPI** (API REST)
- **Streamlit** (dashboard)
- **scikit-learn** (ML)
- **Docker Compose** (infrastructure)

### Structure des donnÃ©es

```
MinIO (data lake)
â”œâ”€â”€ sources/     # CSV bruts
â”œâ”€â”€ bronze/      # DonnÃ©es brutes (CSV)
â”œâ”€â”€ silver/      # DonnÃ©es nettoyÃ©es (Parquet)
â””â”€â”€ gold/        # DonnÃ©es agrÃ©gÃ©es (Parquet)
    â”œâ”€â”€ dimensions/
    â”œâ”€â”€ facts/
    â”œâ”€â”€ kpis/
    â”œâ”€â”€ analytics/
    â””â”€â”€ ml/       # DonnÃ©es enrichies ML

MongoDB
â”œâ”€â”€ kpis_*       # Collections KPIs
â”œâ”€â”€ facts_*      # Collections faits
â”œâ”€â”€ analytics_*  # Collections analytics
â””â”€â”€ metadata_refresh  # MÃ©tadonnÃ©es de refresh
```

---



## (non implÃ©mentÃ©)

- **Kafka + Spark Streaming** : Streaming temps rÃ©el avec dÃ©tection d'anomalies
- **Metabase** : Dashboard BI connectÃ© Ã  MongoDB (bonus)

---


